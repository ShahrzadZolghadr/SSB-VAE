# SSB-VAE: Self-Supervised Bernoulli Autoencoders for Semi-Supervised Hashing

This repository contains the code to reproduce the results presented in the paper 
*Self-Supervised Bernoulli Autoencoders for Semi-Supervised Hashing*.

# Description

We investigate the robustness of hashing methods based on variational autoencoders 
to the lack of supervision, focusing on two semi-supervised approaches currently in use. 
The first augments the training objective of the variational autoencoder to jointly model 
the distribution over the data and the class labels. The second approach exploits the 
annotations to define an additional *pairwise* loss that enforces a consistency 
between the similarity in the code (Hamming) space and the similarity in the label space. 
Our experiments on text and image retrieval tasks show that, as expected, both methods 
can significantly increase the quality of the hash codes. The pairwise approach can exhibit 
an advantage when the number of labelled points is large. However, we found that this method 
can degrade quickly and loose its advantage when the amount of labelled samples decreases. 
To circumvent this problem, we propose a novel supervision method in which the model uses 
its own predictions of the label distribution to implement the pairwise objective. Compared to the best baseline, this procedure yields similar performance in 
fully-supervised settings but improves significantly the results when labelled data is scarce.



# Usage

The code is organised in four different scripts, one per dataset. 
Specifically, the scripts *test_model_[**data**].py* considers the dataset **data** and take as input 
the following parameters:


- *M* is the index of the model considered. In particular, we compare three semi-supervised
 methods based on variational autoencoders: *(M=1)* **VDHS-S** is a variational autoencoder 
 proposed in [[1]](#1) that employs Gaussian latent variables, unsupervised learning and pointwise supervision; 
 *(M=2)* **PHS-GS** is a variational autoencoder proposed in [[2]](#2) that assumes Bernoulli latent variables, 
 unsupervised learning, and both pointwise and pairwise supervision; 
 and *(M=3)* **SSB-VAE** is our proposed method based on Bernoulli latent variable, unsupervised learning, pointwise 
 supervision and self-supervision.

- *p* is the level (percentage) of supervision to consider when training the autoencoder based on semi-supervised approach.
- *v* ??
- The parameters *a*, *b* corresponds to the hyperparameters  α, β of the the final objective function 
for training the autoencoder in semi-supervised settings (Equation 7) 
- *g* ??
- *r* is the number of experiments to perform, given a set of parameters. This is used to compute an average performance
considering multiple initialisation of the same neural network. Notice that the results reported in the paper are 
computing by averaging *r=5* experiments.
- *l* is the size of the laten sub-space generated by the encoder. This also corresponds to the number of bits of 
the generated hash codes.

The script utils.py is used to import the needed packages and all of the custom routines for performance evaluation.

The script base_networks.py contains the custom routines to define all the components of a neural networks.

The script supervised_BAE.py defines the three types of autoencoder (*VDSH, PHS. SSB-VAE*).

The script post_processing.py allows to collect all the results provided by the *.sh files and it computes the
 tables as reported in the paper.
 
 Finally the *.sh files allow to run the all the experiments reported in the paper. In particular, 
 the test_all_[**data**]-[**n**]bits.sh compute *r* times the prediction of the three methods (*VDSH, PHS. SSB-VAE*)
 for *p = 0.1, 0.2, ... , 0.9, 1.0*

[--> Tuning parameters](#)

[--> Requirements: tf2.0.0](#)


## References
<a id="1">[1]</a> 
 S. Chaidaroon and Y. Fang. “Variational deep semantic hashing for text documents”. SIGIR. 2017, pp. 75–84. 

<a id="1">[2]</a>  S. Z. Dadaneh et al. “Pairwise Supervised Hashing with Bernoulli Variational Auto-Encoder and Self-Control Gradient Estimator”. Proc. UAI. 2020